<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Academic Project Page</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Learning Where to Look for Robots: Gaze-Guided Perception for Long-Horizon Robot Learning </h1>
            <h2 class="subtitle is-4 publication-subtitle">UCLA Summer Undergraduate Research Program</h2>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
                   <span class="author-block">
                  <a href="https://www.linkedin.com/in/yike-shi2004/" target="_blank">Yike Shi*</a>,</span>
                  <span class="author-block">
                  <a href="https://www.linkedin.com/in/jon-ouyang/" target="_blank">Jonathan Ouyang*</a>,</span>
                 
                  <span class="author-block">
                    <a href="https://www.linkedin.com/in/yuchen-cui-4702a548/" target="_blank">Yuchen Cui</a><sup></sup>,</span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">University of California, Los Angeles (UCLA)<br>Robot Intelligence Lab</span>
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <!-- <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span> -->

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/UCLA-Robot-Intelligence-Lab/Gaze-ALOHA" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                    </a>
                  </span>
                  <!-- Google Doc link 
                  <span class="link-block">
                    <a href="https://docs.google.com/document/d/1huSfL79zdWfOIvUaqzXI1XtuKO4zQUFc4tvWE-A_qbI/edit?usp=sharing"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-alt"></i>
                    </span>
                    <span>CS 188 Final Report</span>
                    </a>
                  </span>
                  -->
                <!-- <span class="link-block">
                  <a href="https://www.kaggle.com/datasets/jonouyang/sjsu-d1-swimming-dataset" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-kaggle"></i>
                    </span>
                    <span>Dataset</span>
                  </a>
                </span> -->

                <!-- ArXiv abstract Link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="subtitle has-text-centered">
        This work is foundational for our future works at the UCLA Robot Intelligence Lab (URIL). We build this architecture with intent to eventually submit our project to ICRA 2026 
      </h2>
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/Success_Long_H.mp4"
        type="video/mp4">
      </video>
      
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Problem Statemet -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Problem Statement</h2>
        <div class="content has-text-justified">
          <p>
            End-to-End Robot Learning has emerged as an effective paradigm in contemporary robotics, largely propelled by advancements in computational power, computer vision, and AI architectures. Imitation Learning(IL) enables teaching robots intricate, human-like skills. Nonetheless, conventional IL are confronted with challenges, notably multimodality, covariance shift, and error compounding. Strategies from prior research like multi-head transformer ACT and diffusion policies typically impose stringent requirements for high-quality demonstrations—often necessitating hundreds of precise and diverse expert trajectories. Furthermore, their applicability is frequently restricted to sequential actions, and their performance notably degrades as the task horizon expands. How can we achieve long-horizon robot learning with limited demonstrations and multimodal data? 
            This project aims to address this question by GAZE.
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Problem Statement -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Inspired from the human learning principles: complex tasks are decomposed into series of manageable subtasks and learning involves dynamic, selective attention to environmental cues while filtering out irrelevant information, we propose a novel and robust approach to robot imitation learning: gaze-attention-guided policy learning. Our methodology starts by directly capturing human gaze during expert demonstrations. We train a powerful gaze generative model, which predict precisely where the robot should focus its attention(gaze) at inference. We introduce a powerful mechanism that integrates this predictive gaze information, guiding the robot's policy model to discern the specific subtask currently being executed. Our experimental results demonstrate that the incorporation of our generative gaze model enables the learning of non-sequential actions from a more flexible set of demonstrations, and show better performance on long-horizon tasks.

            Key Words: Robotics, Imitation Learning, Human-Robot Interaction, Egocentric Vision
          </p>
            <img src="static/images/Demo-Photoroom.jpg">
            <figcaption>Figure 1: Getting Gaze Data from Aria when teleoperate Robot Arms to finish Tasks</figcaption>
          </figure>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Methodology</h2>
        <div class="content has-text-justified">
          <p>
            We designed a data collection pipeline using GALAXEA’s single-arm teleoperation system to record synchronized multimodal data from a top-down (Logitech) and wrist-mounted (Realsense) camera, along with 7-DOF joint positions at 50 Hz. Each demonstration was about 45 seconds long, and we collected 37 total. To address temporal misalignment between data streams, we implemented synchronization in the PyTorch dataloader using a ±0.02s window. 
          </p>
          <video poster="" id="tree" autoplay controls muted loop height="100%">
            <!-- Your video here -->
            <source src="static/videos/collection.mp4"
            type="video/mp4">
          </video>
          <p>
            We trained a gaze prediction model with 2 ways: a MDN(Multi-Density-Network) + GMM(Gaussian Mixture Model) and a UNet. The MDN model predicts a distribution over gaze points, while the UNet predicts gaze heatmap. We then used the predicted gaze to guide the policy model.
          </p>
          <img src="static/images/MDN.jpg">
            <figcaption>Figure 2: MDN+GMM Predicted Gaze. X is the means of gaussian blobs</figcaption>
          </figure>
          <video
            src="static/videos/gaze_prediction.mp4"
            autoplay
            muted
            loop
            playsinline
          ></video>
          <img src="static/images/UNet.jpg">
            <figcaption>Figure 3: UNet Predicted Gaze. Red stands for high confidence region. Green is the most confident point(gaze point)</figcaption>
          </figure>
          <p>
            The noisy nature of gaze data led us to think about using them as a guiding signal for the policy model. Inspired by human brain, where different regions are responsible for different tasks, we hypothesized that gaze can be used to guide the policy model to focus on the right subtask. We trained multiple Vanilla ACT(Action Chunking Transformer), each represents a skill. We then trained a Gaze-foveated Selector, which predicts which subtask to execute and which policy model to use. A Done Token is also predicted to indicate the end of a subtask. 
          </p>
          <img src="static/images/Selector-1.jpg">
          <img src="static/images/Selector-2.jpg">
            <figcaption>Figure 4: Selector I/O sample and architecture</figcaption>
          </figure>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<style>
.video-carousel-container {
  display: flex;
  gap: 1rem;
  overflow-x: auto;
  padding-bottom: 1rem;
}

.video-item {
  flex: 0 0 auto;
  width: 30%;
  min-width: 200px;
  display: flex;
  flex-direction: column;
  align-items: center;
}

.video-item video {
  width: 100%;
  aspect-ratio: 9 / 16;
  object-fit: cover;
  border-radius: 0.25rem;
}
</style>

<!--Reference citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">References</h2>
      <pre><code>
Zhao, T. Z., Kumar, V., Levine, S., & Finn, C. (2023). Learning fine-grained bimanual manipulation with low-cost hardware. arXiv. https://doi.org/10.48550/arXiv.2304.13705
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->

