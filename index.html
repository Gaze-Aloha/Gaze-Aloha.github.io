<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Academic Project Page</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">CS 188 Final Project: ALOHA ACT Implementaiton</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
                  <span class="author-block">
                  <a href="https://www.linkedin.com/in/jon-ouyang/" target="_blank">Jonathan Ouyang*</a>,</span>
                  <span class="author-block">
                  <a href="https://www.linkedin.com/in/yike-shi2004/" target="_blank">Yike Shi*</a><sup></sup>,</span>
                  <!-- <span class="author-block">
                    <a href="https://www.linkedin.com/in/yuchen-cui-4702a548/" target="_blank">Yuchen Cui</a>
                  </span> -->
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">University of California, Los Angeles (UCLA)<br>Robot Intelligence Lab</span>
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <!-- <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span> -->

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/UCLA-Robot-Intelligence-Lab/Gaze-ALOHA" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                    </a>
                  </span>

                  <span class="link-block">
                    <a href="https://docs.google.com/document/d/1huSfL79zdWfOIvUaqzXI1XtuKO4zQUFc4tvWE-A_qbI/edit?usp=sharing"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-alt"></i>
                    </span>
                    <span>CS 188 Final Report</span>
                    </a>
                  </span>

                <!-- <span class="link-block">
                  <a href="https://www.kaggle.com/datasets/jonouyang/sjsu-d1-swimming-dataset" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-kaggle"></i>
                    </span>
                    <span>Dataset</span>
                  </a>
                </span> -->

                <!-- ArXiv abstract Link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="subtitle has-text-centered">
        This work is foundational for our future works at the UCLA Robot Intelligence Lab (URIL). We build this architecture with intent to eventually submit our project to ICRA 2026 
      </h2>
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/working1.mp4"
        type="video/mp4">
      </video>
      
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Problem Statemet -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Problem Statement</h2>
        <div class="content has-text-justified">
          <p>
            Precise bimanual manipulation tasks—such as stacking, inserting, and threading—have traditionally required high-fidelity robots with expensive sensors and precise calibration. The ALOHA framework introduced a breakthrough in this space by leveraging imitation learning techniques, particularly Action Chunking and Temporal Ensembling, to enable low-cost robots to perform such tasks with minimal demonstration data. However, ALOHA’s success heavily relied on rich multi-view visual input and a dual-arm setup with wrist-mounted cameras. At the UCLA Robotics and Intelligent Learning (URIL) lab, we aim to replicate and adapt the ALOHA framework for single-arm manipulation using GALAXEA’s low-cost teleoperation hardware. Our setup lacks a front-facing camera and uses only one follower arm with a single wrist-mounted view. This project focuses on evaluating the viability of ACT-based imitation learning under constrained sensing and hardware conditions, using a simplified stacking task as a benchmark.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Problem Statement -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            This project explores the feasibility of adapting the ALOHA imitation learning framework to a constrained single-arm robotic setup. Unlike the original multi-view, bimanual ALOHA system, our implementation uses only a bird-view and wrist-mounted camera, along with a single follower arm lacking wrist-mounted sensing on the opposing side. We collected 20 expert demonstrations via teleoperation, capturing synchronized multimodal data at 50 Hz. An Action Chunking with Transformers (ACT) model was trained as a conditional variational autoencoder to predict future joint trajectories based on two camera views and past joint states. Despite hardware and data limitations, the trained policy showed smooth motion and partial task understanding but struggled with object manipulation due to occlusions and visual ambiguity. Our findings suggest that ACT-based learning retains potential even under reduced sensory input, and performance could be significantly improved with more demonstrations, better calibration, and enhanced viewpoint coverage.
          </p>
          <figure>
            <img src="static/images/ACT1.png">
            <figcaption>Figure 1: the original ACT architecture with 4 cameras from the ALOHA paper we base our work off of</figcaption>
          </figure>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Methodology</h2>
        <div class="content has-text-justified">
          <p>
            We designed a data collection pipeline using GALAXEA’s single-arm teleoperation system to record synchronized multimodal data from a top-down (Logitech) and wrist-mounted (Realsense) camera, along with 7-DOF joint positions at 50 Hz. Each demonstration was about 18 seconds long, and we collected 20 total. To address temporal misalignment between data streams, we implemented synchronization in the PyTorch dataloader using a ±0.02s window. 
          </p>
          <video poster="" id="tree" autoplay controls muted loop height="100%">
            <!-- Your video here -->
            <source src="static/videos/training.mp4"
            type="video/mp4">
          </video>
          <p>
            For policy learning, we adapted the ACT model as a Conditional Variational Autoencoder: an encoder compresses joint and action history into a latent variable z, and a transformer-based decoder predicts future joint trajectories from images, joints, and 
𝑧
z. The model uses ResNet-18 for image encoding, 4 transformer encoder layers, 7 decoder layers, and is optimized with AdamW over 1000 epochs using an L1 + KL divergence loss.


          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Discussion & Limitations</h2>
        <div class="content has-text-justified">
          <p>
            Our results highlight both the promise and challenges of imitation learning under constrained setups. The two-camera configuration lacked the full spatial awareness provided in ALOHA’s four-view system, and occlusions in the bird-view camera reduced perceptual consistency. Our dataset of 20 demonstrations was also likely too small to ensure generalization, especially in the presence of hardware noise and offset mismatches between teleoperation and execution. Additionally, running inference at only 30 Hz (due to camera limitations) may have introduced latency. 
          </p>
          <video poster="" id="tree" autoplay controls muted loop height="100%">
            <!-- Your video here -->
            <source src="static/videos/bad.mp4"
            type="video/mp4">
          </video>
          <p>
            Despite these issues, the ACT framework showed potential, and we believe performance can be significantly improved with more data, better calibration, and enhanced visual coverage.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Failure Cases -->
<h2 class="has-text-centered title is-3 mt-3">Failure Cases</h2><section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="video-carousel-container">
        <div class="video-item">
          <video
            src="static/videos/c1.mp4"
            autoplay
            muted
            loop
            playsinline
          ></video>
        <p class="subtitle is-size-6 has-text-centered">
          Robot appears to attempt to go to red object location
        </p>        
        </div>
        <div class="video-item">
          <video
            src="static/videos/c2.mp4"
            autoplay
            muted
            loop
            playsinline
          ></video>
          <p class="subtitle is-size-6 has-text-centered">
          Policy has been severely overfit, robot no longer reacts to state changes
          </p> 
        </div>
        <div class="video-item">
          <video
            src="static/videos/c3.mp4"
            autoplay
            muted
            loop
            playsinline
          ></video>
          <p class="subtitle is-size-6 has-text-centered">
          Model displays odd oscillating behavior
          </p> 
        </div>
      </div>
    </div>
  </div>
</section>

<style>
.video-carousel-container {
  display: flex;
  gap: 1rem;
  overflow-x: auto;
  padding-bottom: 1rem;
}

.video-item {
  flex: 0 0 auto;
  width: 30%;
  min-width: 200px;
  display: flex;
  flex-direction: column;
  align-items: center;
}

.video-item video {
  width: 100%;
  aspect-ratio: 9 / 16;
  object-fit: cover;
  border-radius: 0.25rem;
}
</style>

<!--Reference citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">References</h2>
      <pre><code>
Zhao, T. Z., Kumar, V., Levine, S., & Finn, C. (2023). Learning fine-grained bimanual manipulation with low-cost hardware. arXiv. https://doi.org/10.48550/arXiv.2304.13705
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->

